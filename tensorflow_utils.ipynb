{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")\n",
    "\n",
    "\n",
    "# When debugging, use the follwing to use eager execution.\n",
    "tf.config.run_functions_eagerly(True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/hub/overview?hl=en\n",
    "\n",
    "!pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TF Hub model in Keras https://www.tensorflow.org/hub/tf2_saved_model#using_savedmodels_from_tf_hub\n",
    "model = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\n",
    "hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 17:04:59.950084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/agustinsarasua/workspace/grammarly_interview/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-24 17:05:26.491857: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to data/imdb/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Size...: 100%|██████████| 80/80 [00:14<00:00,  5.34 MiB/s]rl]\n",
      "Dl Completed...: 100%|██████████| 1/1 [00:14<00:00, 14.97s/ url]\n",
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to data/imdb/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/datasets/overview\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "(train_ds, test_ds), info = tfds.load(\"imdb_reviews\", \n",
    "                                      split=['train[:1%]','test[:1%]'],\n",
    "                                      shuffle_files=True, \n",
    "                                      data_dir=\"data/imdb/\", \n",
    "                                      as_supervised=True,\n",
    "                                      with_info=True)\n",
    "\n",
    "assert isinstance(train_ds, tf.data.Dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (train_ds\n",
    "            # .map(...)\n",
    "            .cache()\n",
    "            .batch(32))\n",
    "# train_ds = train_ds.batch(32)\n",
    "# train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = (test_ds\n",
    "           .cache()\n",
    "           .batch(32))\n",
    "# test_ds = test_ds.batch(32)\n",
    "# test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(train_ds.cardinality(), test_ds.cardinality())\n",
    "\n",
    "print(info.features['label'].num_classes)\n",
    "print(info.features[\"label\"].names)\n",
    "print(info.features['label'].int2str(0), info.features['label'].str2int('pos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/datasets/performances\n",
    "\n",
    "def normalize_img(image, label):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "\n",
    "ds, ds_info = tfds.load(\n",
    "    'mnist',\n",
    "    split='train',\n",
    "    as_supervised=True,  # returns `(img, label)` instead of dict(image=, ...)\n",
    "    with_info=True,\n",
    ")\n",
    "\n",
    "ds = (\n",
    "  ds\n",
    "  .map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE) # Applying normalization before `ds.cache()` to re-use it.\n",
    "  .cache()\n",
    "  .shuffle(ds_info.splits['train'].num_examples)  # For true randomness, we set the shuffle buffer to the full dataset size.\n",
    "  .batch(128) # Batch after shuffling to get unique batches at each epoch.\n",
    "  .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load entire dataset in memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 11.06 MiB (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to /Users/agustinsarasua/tensorflow_datasets/mnist/3.0.1...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 100%|██████████| 5/5 [00:04<00:00,  1.03 file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset mnist downloaded and prepared to /Users/agustinsarasua/tensorflow_datasets/mnist/3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/datasets/performances\n",
    "(img_train, label_train), (img_test, label_test) = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split=['train', 'test'],\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42800000000.0, 5550000.0, 12)\n",
      "(-530000000000000.0, 0.0, 2)\n"
     ]
    }
   ],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/data/experimental/CsvDataset\n",
    "\n",
    "dataset = tf.data.experimental.CsvDataset(\n",
    "  \"data/example.csv\",\n",
    "  [tf.float32,  # Required field, use dtype or empty tensor\n",
    "   tf.constant([0.0], dtype=tf.float32),  # Optional field, default to 0.0\n",
    "   tf.int32,  # Required field, use dtype or empty tensor\n",
    "  ],\n",
    "  select_cols=[1,2,3]  # Only parse last three columns\n",
    ")\n",
    "\n",
    "for element in dataset.as_numpy_iterator():\n",
    "  print(element)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Pandas Dataframe to tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "training_df: pd.DataFrame = pd.DataFrame(\n",
    "    data={\n",
    "        'feature1': np.random.rand(10),\n",
    "        'feature2': np.random.rand(10),\n",
    "        'feature3': np.random.rand(10),\n",
    "        'target': np.random.randint(0, 3, 10)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(training_df)\n",
    "\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(training_df[['feature1', 'feature2', 'feature3']].values, tf.float32),\n",
    "            tf.cast(training_df['target'].values, tf.int32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "assert isinstance(training_dataset, tf.data.Dataset) \n",
    "\n",
    "for features_tensor, target_tensor in training_dataset:\n",
    "    print(f'features:{features_tensor} target:{target_tensor}', type(features_tensor), features_tensor.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb dsfb sfd', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb dsfb sfd', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb ', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb sad', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb sad', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'vsad  asdas', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb dsfb sfd', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'afbdab fdsb sdb dsf bdsf b sdfb dsfb sfd', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'213 123 31 3 135 1 5341 434 1 412', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'aadsfasd asfd sad fd saf sdaf asdas', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "tf.Tensor(b'123  321 3123 ', shape=(), dtype=string) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "\n",
      " ------------------ \n",
      "\n",
      "Found 2 files belonging to 2 classes.\n",
      "Sentence:  [b'213 123 31 3 135 1 5341 434 1 412\\naadsfasd asfd sad fd saf sdaf asdas\\n123  321 3123 '\n",
      " b'afbdab fdsb sdb dsf bdsf b sdfb dsfb sfd\\nafbdab fdsb sdb dsf bdsf b sdfb dsfb sfd\\nafbdab fdsb sdb dsf bdsf b sdfb \\nafbdab fdsb sdb dsf bdsf b sdfb sad\\nafbdab fdsb sdb dsf bdsf b sdfb sad\\nvsad  asdas\\nafbdab fdsb sdb dsf bdsf b sdfb dsfb sfd\\nafbdab fdsb sdb dsf bdsf b sdfb dsfb sfd\\n']\n",
      "Label: [1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 18:14:45.931801: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-24 18:14:45.983539: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-24 18:14:45.983981: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [2]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.TextLineDataset([\"data/files/file_1.txt\", \"data/files/file_2.txt\"])\n",
    "\n",
    "for tnsr in dataset:\n",
    "    print(tnsr, type(tnsr))\n",
    "\n",
    "print(\"\\n ------------------ \\n\")\n",
    "\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/utils/text_dataset_from_directory\n",
    "# Example: https://www.tensorflow.org/tutorials/load_data/text\n",
    "\n",
    "dataset = tf.keras.utils.text_dataset_from_directory(\n",
    "    \"data/files/from_dir/\",\n",
    "    labels='inferred',\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "for X, y in dataset.take(1):  \n",
    "    print(\"Sentence: \", X.numpy())\n",
    "    print(\"Label:\", y.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"tensorflow-text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as tf_text\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Saving a model\n",
    "\n",
    "model_path = os.path.join(\"models\", \"some_model/1/\")\n",
    "tf.saved_model.save(model, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving model with TF Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Serving with docker\n",
    "# https://www.tensorflow.org/tfx/serving/docker\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "docker run -t --rm -p 8501:8501 \\    \n",
    "    -v \"/Users/agustinsarasua/workspace/grammarly_interview/models/some_model/1/\" \\\n",
    "    -e MODEL_NAME=some_model \\\n",
    "    tensorflow/serving &\n",
    "\n",
    "curl -d '{\"instances\": [\"hola\", \"This was an absolutely terrible movie. Don'\\''t be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie'\\''s ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor'\\''s like Christopher Walken'\\''s good name. I could barely sit through it\"]}' \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -X POST http://localhost:8501/v1/models/some_model:predict    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard\n",
    "\n",
    "\n",
    "https://www.tensorflow.org/tensorboard/get_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.de import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "# Profiling\n",
    "\n",
    "!pip install -U tensorboard_plugin_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-kernel",
   "language": "python",
   "name": "my-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
